{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Elasticsearch for Spatial Analysis\n",
    "\n",
    "<span style=\"background:yellow\">Introduction & background on Elasticsearch and document databases...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Launch Elasticsearch locally using Docker Compose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch contains multiple services/components that need to communicate with each other.  This is hard to accomplish when using isolated Docker containers, as these containers are generally not set up to be mutually accessible to each other.  Instead, it is easier to use Docker Compose, a container orchestration utility that allows you to run multiple, linked services within networked containers that can communicate with each other.  This repository contains an `elasticsearch-docker-compose.yml` file that defines the parameters for launching Elasticsearch using Docker Compose.  The basic structure of the file looks like this:\n",
    "\n",
    "```\n",
    "services:\n",
    "  elasticsearch:\n",
    "    image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0\n",
    "    container_name: elasticsearch\n",
    "    ...\n",
    "\n",
    "  kibana:\n",
    "    image: docker.elastic.co/kibana/kibana:6.4.0\n",
    "    container_name: kibana\n",
    "    ...\n",
    "\n",
    "volumes:\n",
    "  ...\n",
    "\n",
    "networks:\n",
    "  ...\n",
    "```\n",
    "\n",
    "The `services` section lists the different services that we are trying to launch and coordinate together; in this case, we'll be launching Elasticsearch (the database itself) and Kibana (Elasticsearch's web-based access interface).  The `volumes` section creates persistent data volume to hold the database files.  The `networks` section creates a virtual networking interface that the Elasticsearch and Kibana containers can use to interface with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To launch the Elasticsearch stack, open up your command line and `cd` into the folder containing the `elasticsearch-docker-compose.yml` file.  Then, run the following command:\n",
    "\n",
    "```bash\n",
    "docker-compose -f elasticsearch-docker-compose.yml up -d\n",
    "```\n",
    "\n",
    "What does this command do?  Breaking it down, here's what each argument means:\n",
    "\n",
    "- **-f** : specifies the filename of the .yml file that describes the cluster of services we want to run\n",
    "- **up** : initializes the containers and launches the services specified in the Docker Compose file\n",
    "- **-d** : tells Docker Compose to run the cluster in detached mode, so it runs in the background even if you quit your console\n",
    "\n",
    "As the stack launches, you should see the following confirmation messages in your command window:\n",
    "\n",
    "![Elasticsearch Docker Compose up message](img_elasticsearch/elasticsearch_up.png)\n",
    "\n",
    "To check that the stack is running, execute the following command in the command line:\n",
    "\n",
    "```docker container ls```\n",
    "\n",
    "You should see something like this, indicating that the services are successfully running in two separate, but \"orchestrated\" containers:\n",
    "\n",
    "```\n",
    "CONTAINER ID    IMAGE               COMMAND                CREATED         STATUS          PORTS\n",
    "blahblahblah    kibana:6.4.0        \"/usr/local/bin/kiba…\" X seconds ago   Up X seconds    0.0.0.0:5601->5601/tcp\n",
    "\n",
    "blahblahblah    elasticsearch:6.4.0 \"/usr/local/bin/dock…\" X seconds ago   Up X seconds    0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Connect to the database\n",
    "\n",
    "Elasticsearch has a web-based interface called Kibana that you can access using a web browser. Launch your web browser of choice and navigate to:\n",
    "\n",
    "```http://localhost:5601/app/kibana```\n",
    "\n",
    "If the Elasticsearch and Kibana containers are successfully initialized and communicating with each other, you should be able to see the following interface:\n",
    "\n",
    "![Kibana browser launch page](img_elasticsearch/kibana_browser_launch.png)\n",
    "\n",
    "You'll notice that this approach does not require you to authenticate or set a username or password.  This is because the Docker Compose file used to launch the cluster is set up to disable authentication features to make it easier to access the database for testing.  You will need to enable an authentication method and take other security precautions to secure the database installation if you want to use Elasticsearch in a production environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Load data into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A) Create an \"index\" and define its mappings\n",
    "\n",
    "Elasticsearch uses the term [\"index\"](https://www.elastic.co/blog/what-is-an-elasticsearch-index) to describe a set of records that are stored together.  This is similar to a \"table\" in a relational database or a \"collection\" in databases like MongoDB.  The term \"index\" is a natural fit, because Elasticsearch was originally invented to power search engine-style projects: its goal was to \"index\" records, web/server logs, or other large datasets so they could be easily searched later.  \n",
    "\n",
    "By default, Elasticsearch \"indexes\" are, well...indexed.  This means that, when reading data in, Elasticsearch will automatically try to detect the field types in your data and store/index the data in a way that can be easily searched later.  This process of reading in data and assigning field types is called [mapping](https://www.elastic.co/blog/found-elasticsearch-mapping-introduction).  Sometimes, Elasticsearch can guess wrong when detecting field types, so it can be helpful to define an explicit mapping--at least for some of the trickier fields--when reading data in.\n",
    "\n",
    "The mapping definition below is the one we'll use for the Twitter data.  It does a few things: first, it explicitly defines some of the more uncommon data types--like the timestamp, centroid, and bounding box--to help Elasticsearch recognize these data types properly.  Second, the `\"_source\": { \"enabled\": true }` setting specifies that we should save the original JSON representing the tweet when reading it into the database.  If we don't enable [source mapping](https://www.elastic.co/guide/en/elasticsearch/reference/6.x/mapping-source-field.html) like this, Elasticsearch will only store the metadata of the tweet, and will not allow you to conduct any deep search or retrieval on the full tweet.  It can be useful to forego source mappings when you are indexing large files (such as raster images or other multimedia) and only wish to use Elasticsearch as a metadata organizer, and will be storing the actual source data elsewhere.  This, however, is not the case with our tweets, which are simple text data and thus don't have a large storage overhead.  So in this scenario, you will want to enable source mapping so you can query the body of the tweets after they have been stored in Elasticsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PUT twitter_sample\n",
    "{\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"tweet\": {\n",
    "            \"_source\": { \"enabled\": true },\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\" },\n",
    "                \"timestamp_ms\": { \"type\": \"date\", \"format\": \"epoch_millis\" },\n",
    "                \"user\": { \n",
    "                    \"properties\": { \n",
    "                        \"location\": { \"type\": \"text\" },\n",
    "                        \"description\": { \"type\": \"text\" }\n",
    "                    }\n",
    "                },\n",
    "                \"place\": { \n",
    "                    \"properties\": { \n",
    "                        \"name\": { \"type\": \"keyword\" },\n",
    "                        \"full_name\": { \"type\": \"keyword\" },\n",
    "                        \"centroid\": { \"type\": \"geo_shape\" },\n",
    "                        \"better_bounding_box\": { \"type\": \"geo_shape\" },\n",
    "                        \"centroid_geohash\": { \"type\": \"geo_point\" }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading any data, copy the mapping definition shown above.  Then, navigate to the \"Dev Tools\" section in Kibana.  Copy and paste the mapping definition into the console and click the green \"play\" button to set up the mapping:\n",
    "\n",
    "![Elasticsearch mapping](img_elasticsearch/elasticsearch_mapping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B) Execute load scripts\n",
    "\n",
    "Now it's time to load in the data. We'll use tweets that come in the form of .txt files that have been split into chunks of ~5000 tweets per file. Each line of each file is formatted as a JSON object representing a single tweet, and each tweet is separated by a newline within the .txt files. To make loading easier, this repository contains a pre-baked script called Clean_Load_Scripts.py that you can use to load data into Elasticsearch.  Run the following code to import the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Clean_Load_Scripts as cleanNLoad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a variable called data_folder that points to the location of the demo data on your computer. Also define a variable called logs_folder that points to the path where you want to store log files about the load. This folder can be located directly inside of your data folder, or it can be an entirely separate path. The load script will keep track of files that need to be loaded, load time for each file, error counts, and some other diagnostic data for each file as it loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/path/to/data/twitter_data/data_small_5000_split/'\n",
    "logs_folder = '/path/to/data/twitter_data/data_small_5000_split/logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, initialize the extractor to kick off the load logs and prep the files for load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = cleanNLoad.Extractor(data_folder, logs_folder, initialize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the `while` loop below that actually does the work of cleaning and loading each file into the database. \n",
    "\n",
    "If the while loop is interrupted for some reason during the load, don't panic! The logs folder contains a files_to_load.txt file that keeps track of all the files that still need to be loaded into the database. To re-start the load, simply re-load the extractor above with the argument initialize=False. Then, re-run the while loop below and the load will pick up where it left off.\n",
    "\n",
    "Ready to load? Okay, go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor: Next file is: twitter_sample_5GB_split_ab.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ab.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bh.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bh.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cn.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cn.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_aw.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_aw.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_an.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_an.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bq.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bq.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bd.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bd.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cb.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cb.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_az.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_az.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cc.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cc.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_be.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_be.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bp.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bp.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ao.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ao.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_co.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_co.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_av.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_av.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bi.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bi.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ac.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ac.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cq.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cq.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ah.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ah.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bw.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bw.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bb.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bb.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cd.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cd.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ad.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ad.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bn.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bn.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_aq.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_aq.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ch.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ch.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ap.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ap.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ci.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ci.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bo.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bo.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bz.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bz.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ae.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ae.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ce.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ce.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bc.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bc.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bv.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bv.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cp.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cp.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ai.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ai.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cf.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cf.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bu.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bu.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_aj.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_aj.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cj.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cj.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_as.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_as.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bl.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bl.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_by.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_by.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_af.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_af.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ag.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ag.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bx.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bx.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bm.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bm.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ck.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ck.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ar.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ar.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ak.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ak.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cr.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cr.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bt.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bt.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ba.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ba.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cg.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cg.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_au.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_au.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cl.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cl.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bj.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bj.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ay.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ay.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bf.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bf.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bs.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bs.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_al.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_al.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_am.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_am.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_br.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_br.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bg.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bg.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ca.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ca.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_ax.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_ax.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_aa.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_aa.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_bk.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_bk.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_at.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_at.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: twitter_sample_5GB_split_cm.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_20000_split/twitter_sample_5GB_split_cm.json\n",
      "Extractor: Read 20000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_sample\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n"
     ]
    }
   ],
   "source": [
    "while extractor.next_file_available():\n",
    "    next_file_data, next_file_name = extractor.get_next_file() # read in the next file\n",
    "    cleaner = cleanNLoad.Cleaner(next_file_data, next_file_name, logs_folder) # clean the data (fix bounding boxes, add centroids, etc.)\n",
    "    cleaned_data = cleaner.clean_data() \n",
    "    loader = cleanNLoad.Loader(cleaned_data, next_file_name, logs_folder) # initialize the loader\n",
    "    loader.get_connection(\"elasticsearch\", \"localhost\", \"9200\", db_name=\"twitter_sample\") # create a database connection\n",
    "    loader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it will take ~1 minute to load each file, so be ready grab a coffee and be patient.  The load time takes longer in Elasticsearch than it does a database like MongoDB because the fields are being indexed and optimized for search as the load is occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Query the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4A) Basic queries\n",
    "\n",
    "Now that the data is loaded, let's look at how to perform a few basic queries. Elasticsearch uses a query syntax that looks a little bit like a combination of a REST call used in making web page requests and a JSON object used to define data structures for the web.  Each query starts with the verb \"GET\", and then contains a series of nested brackets `[]` or `{}` that define the query parameters.\n",
    "```\n",
    "GET twitter_sample/_search\n",
    "{   \"_source\": [\"text\"],\n",
    "    \"query\": {\n",
    "        \"match\" : {\n",
    "            \"text\" : \"search text goes here\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "As you can see above, the beginning of the query starts with a `GET` request, along with the \"index\" (database) name, plus the `_search` operator.  The next line, the `\"_source\"` specification, is where you can define which fields from the original tweet data you want to return in your results set.  Then, the body of the query itself is contained entirely in brackets `{}`.  \n",
    "\n",
    "You can copy and paste queries into the Kibana Dev Tools console and click the green \"run\" button that appears in the upper right corner of each query to run that particular query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Query 1: Match\n",
    "\n",
    "One of the basic query operators available in Elasticsearch is the \"match\" operator.\n",
    "\n",
    "```\n",
    "GET twitter_sample/_search\n",
    "{ \"_source\": [\"user.screen_name\", \"text\"],\n",
    "  \"query\": {\n",
    "    \"match\": {\n",
    "      \"text\": \"blessed\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "The results are ordered by score, but what exactly is this \"score\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Basic Query 2: Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Basic Query 3: Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B) Spatial queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4C) Advanced Queries\n",
    "\n",
    "Text searches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* Install Elasticsearch with Docker. [Elasticsearch documentation] https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html\n",
    "\n",
    "* Learning Elasticstack. [Packt Publishing]\n",
    "\n",
    "* Building an Elasticstack Index with Python. https://qbox.io/blog/building-an-elasticsearch-index-with-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
