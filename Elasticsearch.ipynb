{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Elasticsearch for Spatial Analysis\n",
    "\n",
    "<span style=\"background:yellow\">Introduction & background on Elasticsearch and document databases...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Elasticsearch locally using Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch contains multiple services/components that need to communicate with each other.  This is hard to accomplish when using isolated Docker containers, as these containers are generally not set up to be mutually accessible to each other.  Instead, it is easier to use Docker Compose, a container orchestration utility that allows you to run multiple, linked services within networked containers that can communicate with each other.\n",
    "\n",
    "```bash\n",
    "docker-compose -f elasticsearch-docker-compose.yml up -d\n",
    "```\n",
    "\n",
    "-f specifies the filename of the .yml file that describes the cluster of services we want to run\n",
    "\n",
    "-d tells Docker Compose to run the cluster in detached mode, so it runs in the background even if you quit your console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "```bash\n",
    "docker volume create elasticsearch_volume\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create an index and define its mappings\n",
    "\n",
    "PUT twitter_sample\n",
    "{\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"tweet\": {\n",
    "            \"_source\": { \"enabled\": true },\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\" },\n",
    "                \"timestamp_ms\": { \"type\": \"date\", \"format\": \"epoch_millis\" },\n",
    "                \"user\": { \n",
    "                    \"properties\": { \n",
    "                        \"location\": { \"type\": \"text\" },\n",
    "                        \"description\": { \"type\": \"text\" }\n",
    "                    }\n",
    "                },\n",
    "                \"place\": { \n",
    "                    \"properties\": { \n",
    "                        \"name\": { \"type\": \"keyword\" },\n",
    "                        \"full_name\": { \"type\": \"keyword\" },\n",
    "                        \"centroid\": { \"type\": \"geo_shape\" },\n",
    "                        \"better_bounding_box\": { \"type\": \"geo_shape\" },\n",
    "                        \"centroid_geohash\": { \"type\": \"geo_point\" }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute load scripts\n",
    "\n",
    "<span style=\"background:yellow\">Explain bulk insert functionality</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Clean_Load_Scripts as cleanNLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_folder = '/Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_split/'\n",
    "#logs_folder = '/Users/linkalis/Desktop/twitter_data/twitter_sample_5GB_split/logs/'\n",
    "\n",
    "data_folder = '/Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/'\n",
    "logs_folder = '/Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = cleanNLoad.Extractor(data_folder, logs_folder, initialize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor: Next file is: 500M_unicode_splitac.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitac.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitav.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitav.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbi.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbi.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitao.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitao.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitaz.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitaz.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbe.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbe.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbd.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbd.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitan.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitan.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbh.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbh.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitaw.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitaw.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitab.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitab.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitai.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitai.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbc.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbc.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitae.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitae.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitap.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitap.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbn.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbn.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitaq.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitaq.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitad.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitad.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbb.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbb.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitah.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitah.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitba.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitba.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitak.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitak.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbm.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbm.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitar.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitar.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitag.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitag.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitaf.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitaf.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitas.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitas.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbl.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbl.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitaj.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitaj.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbk.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbk.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitat.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitat.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitaa.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitaa.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbg.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbg.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitax.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitax.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitam.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitam.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splital.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splital.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitay.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitay.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbf.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbf.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitau.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitau.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n",
      "Extractor: Next file is: 500M_unicode_splitbj.json\n",
      "Extractor: Reading file: /Users/linkalis/Desktop/twitter_data/twitter_sample_500MB_5000_split/500M_unicode_splitbj.json\n",
      "Extractor: Read 5000 data rows.\n",
      "Cleaner: Finished cleaning records.\n",
      "Connected to ElasticSearch instance.\n",
      "Checking for existence of index called: twitter_small_with_source\n",
      "Loader: Loading records...\n",
      "Loader: Finished loading records.\n"
     ]
    }
   ],
   "source": [
    "while extractor.next_file_available():\n",
    "    next_file_data, next_file_name = extractor.get_next_file() # read in the next file\n",
    "    cleaner = cleanNLoad.Cleaner(next_file_data, next_file_name, logs_folder) # clean the data (fix bounding boxes, add centroids, etc.)\n",
    "    cleaned_data = cleaner.clean_data() \n",
    "    loader = cleanNLoad.Loader(cleaned_data, next_file_name, logs_folder) # initialize the loader\n",
    "    loader.get_connection(\"elasticsearch\", \"localhost\", \"9200\", db_name=\"twitter_small_with_source\") # create a database connection\n",
    "    #loader.load_batch_data() # load the file's data as a batch\n",
    "    loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Advanced Queries\n",
    "\n",
    "Text searches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* Install Elasticsearch with Docker. [Elasticsearch documentation] https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html\n",
    "\n",
    "* Learning Elasticstack. [Packt Publishing]\n",
    "\n",
    "* Building an Elasticstack Index with Python. https://qbox.io/blog/building-an-elasticsearch-index-with-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
